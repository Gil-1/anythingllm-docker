version: "3.8"
services:
  anythingllm:
    image: mintplexlabs/anythingllm
    container_name: anythingllm
    pull_policy: always
    tty: true
    ports:
      - "3001:3001"
    cap_add:
      - SYS_ADMIN
    environment:
      - STORAGE_DIR=/app/server/storage
      - JWT_SECRET=${JWT_SECRET}
      - LLM_PROVIDER=ollama
      - OLLAMA_BASE_PATH=http://ollama:11434
      - OLLAMA_MODEL=llama3:8b-instruct-q5_1
      - OLLAMA_MODEL_TOKEN_LIMIT=2048
      - EMBEDDING_ENGINE=ollama
      - EMBEDDING_MODEL=nomic-embed-text
      - EMBEDDING_BASE_PATH=http://ollama:11434
      - EMBEDDING_MODEL_PREF=nomic-embed-text:latest
      - EMBEDDING_MODEL_MAX_CHUNK_LENGTH=8192
      - VECTOR_DB=lancedb
      - CHROMA_ENDPOINT=http://chroma:8000
      - CHROMA_API_HEADER=
      - CHROMA_API_KEY=
      - GENERATE_JWT_SECRET=false
      - WHISPER_PROVIDER=local
      - TTS_PROVIDER=native
      - PASSWORDMINCHAR=8
    volumes:
      - anythingllm_storage:/app/server/storage
      - anythingllm_chroma:/app/server/storage/vector-db/chroma
    restart: unless-stopped
    depends_on:
      - ollama
      - chroma
    networks:
      - anythingllm-network

  # NVIDIA GPU Configuration for Ollama
  ollama:
    image: ollama/ollama:latest
    container_name: any-ollama
    pull_policy: always
    tty: true
    restart: unless-stopped
    runtime: nvidia
    environment:
      - OLLAMA_KEEP_ALIVE=24h
    volumes:
      - anythingllm_ollama:/root/.ollama
    ports:
      - "11434:11434"
    networks:
      - anythingllm-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  chroma:
    image: ghcr.io/chroma-core/chroma:latest
    volumes:
      - anythingllm_chroma_data:/chroma/chroma
    environment:
      - ALLOW_RESET=true
      - ANONYMIZED_TELEMETRY=false
    ports:
      - "8000:8000"
    networks:
      - anythingllm-network
    restart: unless-stopped

networks:
  anythingllm-network:
    driver: bridge

volumes:
  anythingllm_chroma:
    driver: local
  anythingllm_ollama:
    driver: local
  anythingllm_chroma_data:
    driver: local

